Vellum Workflows help you quickly prototype, deploy, version, and monitor complex chains of LLM calls and the business logic that tie them together.

It provides a low-code interface for defining these chains so that you get rapid feedback on how they work across a variety of test cases that you define. Once you’re happy with the Workflow, you can “deploy” it and hit an API to invoke that Workflow from your application.

Once deployed, future changes to the Workflow definition are versioned and invocations made from your application are logged. For a given invocation, you can view the inputs, outputs, and latency of each step along the way.

# Concepts

Workflows make heavy use of the following concepts:

1. Input Variables
2. Scenarios
3. Nodes
4. Edges
5. Final Outputs

Let’s take a look at each

### Input Variables

The behavior of most Workflows depend on 1 or more dynamic Input. For example, you could define a single Input named `query` that your Workflow depends on.
