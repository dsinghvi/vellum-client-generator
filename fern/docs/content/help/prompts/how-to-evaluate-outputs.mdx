Evaluating model quality is challenging because the responses generated by LLMs is inherently
probabilistic when temperature is non-zero. Testing output quality before it goes in production
(unit testing) and testing while making changes in production (regression testing) are both
very important to ensure a high quality end user experience. We’ve covered our perspective on testing
LLM quality in our blog [here](https://www.vellum.ai/blog/how-to-evaluate-the-quality-of-large-language-models-for-production-use-cases)

There are 3 kinds of testing you can do in Vellum:

1. Unit testing with 2-10 scenarios (in Playground)
2. Unit testing with more than 10 scenarios (Test Suites)
3. Back-testing while changing prompts in production (Manage)

This article covers Unit testing with 2-10 scenarios, refer to
[this](https://www.notion.so/Testing-in-Bulk-ee1f617be7f94801b7f4a461fb196ad0?pvs=21) article for
test suites and [this](https://www.notion.so/Testing-in-Bulk-ee1f617be7f94801b7f4a461fb196ad0?pvs=21)
article for backtesting.

# Testing output quality in Playground

The Playground is meant for rapid iteration between multiple models/prompts.
We’ve usually seen people spend hours in here iterating on prompts and model
providers until they find a prompt that clears their test cases.

## Setting up

Toggle the Evaluate Outputs button on to start evaluating output quality

![Image](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffa8ed485-aa83-4dcf-958f-9b26a97f8c2c%2FScreen_Shot_2023-05-03_at_3.36.38_PM.png?table=block&id=4c309d1a-2a0f-46fe-a692-7d36fbdb8ae0&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=670&userId=&cache=v2)
